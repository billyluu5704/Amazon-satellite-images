{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential, load_model, save_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import mean, std\n",
    "import os\n",
    "from os import listdir\n",
    "from numpy import save\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib.image import imread\n",
    "from shutil import copyfile\n",
    "from random import seed, random\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Amazon/train-jpg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    filename = folder + 'train_' + str(i) + '.jpg'\n",
    "    image = imread(filename)\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Mappings\n",
    "filename = 'Amazon/train_v2.csv/train_v2.csv'\n",
    "mapping_csv = read_csv(filename)\n",
    "#summarize properties\n",
    "print(mapping_csv.shape)\n",
    "print(mapping_csv[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create set of labels\n",
    "labels = set()\n",
    "for i in range(len(mapping_csv)):\n",
    "    #convert spaced separated tags into an array of tags\n",
    "    tags = mapping_csv['tags'][i].split(' ')\n",
    "    #add tags to the set of labels\n",
    "    labels.update(tags)\n",
    "    print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert set to list\n",
    "labels = list(labels)\n",
    "#set order\n",
    "labels.sort()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict that m√°p labels to integers and reverse\n",
    "labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "inv_map = {i:labels[i] for i in range(len(labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mapping of tags to integers given loaded mapping file\n",
    "def create_tag_mapping(mapping_csv):\n",
    "    #create set of known tags\n",
    "    labels = set()\n",
    "    for i in range(len(mapping_csv)):\n",
    "        #convert spaced separated tags into array of tags\n",
    "        tags = mapping_csv['tags'][i].split(' ')\n",
    "        #add tags to labels set\n",
    "        labels.update(tags)\n",
    "    #convert set of labels to list\n",
    "    labels = list(labels)\n",
    "    #order set alphabetically\n",
    "    labels.sort()\n",
    "    #dict maps labels to integers and reverse\n",
    "    labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "    inv_map = {i:labels[i] for i in range(len(labels))}\n",
    "    return labels_map, inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Amazon/train_v2.csv/train_v2.csv'\n",
    "mapping_csv = read_csv(filename)\n",
    "#create a mapping of tags to integers\n",
    "mapping, inv_mapping = create_tag_mapping(mapping_csv)\n",
    "print(len(mapping))\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mapping of filename to tags\n",
    "def create_file_mapping(mapping_csv):\n",
    "    mapping = dict()\n",
    "    for i in range(len(mapping_csv)):\n",
    "        name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i]\n",
    "        mapping[name] = tags.split(' ')\n",
    "    return mapping\n",
    "mapping = create_file_mapping(mapping_csv)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create In-Memory Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one hot encoding for 1 list of tags\n",
    "def one_hot_encoding(tags, mapping):\n",
    "    #create empty vector\n",
    "    encoding = np.zeros(len(mapping), dtype='uint8')\n",
    "    #mark 1 for each tag in vector\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all images into memory\n",
    "def load_dataset(path, file_mapping, tap_mapping):\n",
    "    photos, targets = list(), list()\n",
    "    #enumerate files in directory\n",
    "    for filename in listdir(folder):\n",
    "        #load image\n",
    "        photo = load_img(path + filename, target_size=(128,128))\n",
    "        #convert to numpy array\n",
    "        photo = img_to_array(photo, dtype='uint8')\n",
    "        #get tags\n",
    "        tags = file_mapping[filename[:-4]]\n",
    "        #one hot encode tags\n",
    "        target = one_hot_encoding(tags, tap_mapping)\n",
    "        #store\n",
    "        photos.append(photo)\n",
    "        targets.append(target)\n",
    "    X = np.asarray(photos, dtype='uint8')\n",
    "    y = np.asarray(targets, dtype='uint8')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_csv = read_csv(filename)\n",
    "tag_mapping, _ = create_tag_mapping(mapping_csv)\n",
    "#create a mapping of filenames to tag lists\n",
    "file_mapping = create_file_mapping(mapping_csv)\n",
    "#load jpeg images\n",
    "X, y = load_dataset(folder, file_mapping, tag_mapping)\n",
    "print(X.shape, y.shape)\n",
    "#save both arrays to one file in compressed format\n",
    "np.savez_compressed('planet_data.npz', X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train and test dataset\n",
    "def dataset():\n",
    "    #load dataset\n",
    "    data = np.load('planet_data.npz')\n",
    "    X, y = data['arr_0'], data['arr_1']\n",
    "    #seperate into train and test datasets\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = dataset()\n",
    "#make all one predictions\n",
    "train_yhat = np.asarray([np.ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
    "test_yhat = np.asarray([np.ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
    "#evaluate predictions\n",
    "train_score = fbeta_score(trainY, train_yhat, beta=2, average='samples')\n",
    "test_score = fbeta_score(testY, test_yhat, beta=2, average='samples')\n",
    "print(f'All Ones: train={train_score:.3f}, test={test_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate fbeta score for multiclass/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    #convert y_true and y_pred to float32\n",
    "    y_true = backend.cast(y_true, 'float32')\n",
    "    y_pred = backend.cast(y_pred, 'float32')\n",
    "    #clip predictions\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    #calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true*y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred-y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true-y_pred, 0, 1)), axis=1)\n",
    "    #calculate precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    #calculate recall\n",
    "    r = tp / (tp + fn + backend.epsilon())\n",
    "    #calculate fbeta, averaged across each class\n",
    "    bb = beta**2 #beta squared\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = dataset()\n",
    "#make all one predictions\n",
    "train_yhat = np.asarray([np.ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
    "test_yhat = np.asarray([np.ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
    "#evaluate predictions\n",
    "train_score = fbeta_score(trainY, train_yhat, beta=2, average='samples')\n",
    "test_score = fbeta_score(testY, test_yhat, beta=2, average='samples')\n",
    "print(f'All Ones (sklearn): train={train_score:.3f}, test={test_score:.3f}')\n",
    "#evaluate predictions with keras\n",
    "train_score = fbeta(backend.constant(trainY, dtype='float32'), \n",
    "                    backend.constant(train_yhat, dtype='float32'))\n",
    "test_score = fbeta(backend.constant(testY, dtype='float32'),\n",
    "                   backend.constant(test_yhat, dtype='float32'))\n",
    "print(f'All Ones (keras): train={train_score:.3f}, test={test_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Dense, MaxPooling2D, Dropout, Flatten, Layer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "class Identity(Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(Identity, self).__init__(**kwargs)\n",
    "        filter1, filter2, filter3 = filters\n",
    "        self.conv1 = Conv2D(filter1, (1,1))\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.act1 = Activation('relu')\n",
    "        self.conv2 = Conv2D(filter2, (3,3), padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.act2 = Activation('relu')\n",
    "        self.conv3 = Conv2D(filter3, (1,1))\n",
    "        self.bn3 = BatchNormalization()\n",
    "    def call(self, input_tensor, training=False):\n",
    "        #layer1\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.act1(x)\n",
    "        #layer2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.act2(x)\n",
    "        #layer3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = Add()([x, input_tensor])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "class Convolutional(Layer):\n",
    "    def __init__(self, filters, strides=(2,2), **kwargs):\n",
    "        super(Convolutional, self).__init__(**kwargs)\n",
    "        filter1, filter2, filter3 = filters\n",
    "        self.conv1 = Conv2D(filter1, (1,1), strides=strides)\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.act1 = Activation('relu')\n",
    "        self.conv2 = Conv2D(filter2, (3,3), padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.act2 = Activation('relu')\n",
    "        self.conv3 = Conv2D(filter3, (1,1))\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.shortcut_conv = Conv2D(filter3, (1,1), strides=strides)\n",
    "        self.shortcut_bn = BatchNormalization()\n",
    "    def call(self, input_tensor, training=False):\n",
    "        #layer1\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.act1(x)\n",
    "        #layer2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.act2(x)\n",
    "        #layer3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        #shortcut layer\n",
    "        shortcut = self.shortcut_conv(input_tensor)\n",
    "        shortcut = self.shortcut_bn(shortcut, training=training)\n",
    "        x = Add()([x, shortcut])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "class Resnet(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Resnet, self).__init__(**kwargs)\n",
    "        #layer 1\n",
    "        self.conv1 = Conv2D(64, (7, 7), strides=(2, 2), padding='same', input_shape=(128, 128, 3))\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.act1 = Activation('relu')\n",
    "        self.maxpool1 = MaxPooling2D((3, 3), strides=(2,2), padding='same')\n",
    "        self.res_block1 = self.resnet_block((64, 64, 256), 3, strides=(1, 1))\n",
    "        self.res_block2 = self.resnet_block((128, 128, 512), 4, strides=(2, 2))\n",
    "        self.res_block3 = self.resnet_block((256, 256, 1024), 6, strides=(2, 2))\n",
    "        self.res_block4 = self.resnet_block((512, 512, 2048), 3, strides=(2, 2))\n",
    "        self.avgpool = GlobalAveragePooling2D()\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(1000, activation='relu')\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(512, activation='relu')\n",
    "        self.dense3 = Dense(17, activation='sigmoid')\n",
    "    def resnet_block(self, filters, blocks, strides=(2,2)):\n",
    "        res_blocks = []\n",
    "        res_blocks.append(Convolutional(filters, strides))\n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.append(Identity(filters))\n",
    "        return Sequential(res_blocks)\n",
    "    def call(self, x):\n",
    "        #Stage 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        #Stage 2\n",
    "        x = self.res_block1(x)\n",
    "        #stage 3\n",
    "        x = self.res_block2(x)\n",
    "        #Stage 4\n",
    "        x = self.res_block3(x)\n",
    "        #Stage 5\n",
    "        x = self.res_block4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = Resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(input_shape=(128, 128, 3), output_shape=17):\n",
    "    model = Resnet()\n",
    "    opt = Adam()\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', fbeta])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "    #plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(histories.history['loss'], color='blue', label='train')\n",
    "    plt.plot(histories.history['val_loss'], color='orange', label='test')\n",
    "    #plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(histories.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(histories.history['val_fbeta'], color='orange', label='test')\n",
    "    plt.show()\n",
    "    #save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    plt.savefig(filename + '_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_harness():\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    #prepare iterators\n",
    "    train_it = train_datagen.flow(trainX, trainY, batch_size=128)\n",
    "    test_it = test_datagen.flow(testX, testY, batch_size=128)\n",
    "    #load model\n",
    "    filename = 'amazon_model.keras'\n",
    "    model = define_model()\n",
    "    #fit model\n",
    "    history = model.fit(train_it, \n",
    "                        steps_per_epoch=len(train_it) - 1, \n",
    "                        validation_data=test_it, \n",
    "                        validation_steps=len(test_it) - 1, \n",
    "                        epochs=200, verbose=1)\n",
    "    #evaluate model\n",
    "    loss, acc, fbeta = model.evaluate(test_it, \n",
    "                            steps=len(test_it), \n",
    "                            verbose=0) \n",
    "    print(f'loss={loss:.3f}, accuracy={acc:.3f}, fbeta={fbeta:.3f}')\n",
    "    #learning curves\n",
    "    summarize_diagnostics(history)\n",
    "    #save model\n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    #load image\n",
    "    img = load_img(filename, target_size=(128, 128))\n",
    "    #convert to array\n",
    "    img = img_to_array(img)\n",
    "    #reshape into a single sample with 3 channels\n",
    "    img = img.reshape(1, 128, 128, 3)\n",
    "    #center pixel data\n",
    "    img = img.astype('float32')\n",
    "    img -= [123.68, 116.779, 103.939]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert predict to tags\n",
    "def prediction_to_tags(inv_mapping, prediction):\n",
    "    #round probabilities to 0 or 1\n",
    "    values = prediction.round()\n",
    "    #collect all predicted tags\n",
    "    tags = [inv_mapping[i] for i in range(len(values)) if i in inv_mapping and values[i] == 1.0]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(inv_mapping):\n",
    "    folder_path = 'Amazon/test-jpg'\n",
    "    #load model\n",
    "    custom_objects = {'Resnet': Resnet,'fbeta': fbeta}\n",
    "    model = load_model('amazon_model.keras', custom_objects=custom_objects)\n",
    "    #predict\n",
    "    #enumerate files in the directory\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            img = load_image(file_path)\n",
    "            result = model.predict(img)\n",
    "            print(result[0])\n",
    "            #map prediction to tags\n",
    "            tags = prediction_to_tags(inv_mapping, result[0])\n",
    "            print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Amazon/sample_submission_v2.csv/sample_submission_v2.csv'\n",
    "mapping_csv = read_csv(filename)\n",
    "#create mapping of tags to int\n",
    "_, inv_mapping = create_tag_mapping(mapping_csv)\n",
    "#entry point, run example\n",
    "run_example(inv_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
